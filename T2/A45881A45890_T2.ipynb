{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Network without Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# load data\n",
    "D = pickle.load(open(\"xorData.p\", \"rb\"))\n",
    "# print(D)\n",
    "\n",
    "X = D.data - 0.5  # Compensate the inputs offset\n",
    "y = D.target\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# print(X1[:,0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X[0, y == 0], X[1, y == 0], '.')\n",
    "plt.plot(X[0, y == 1], X[1, y == 1], '.')\n",
    "# plt.plot(X[0, :], X[1, :], '.')\n",
    "# plt.plot(y1, y1, '.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training algorithm a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    hidden_units=10,\n",
    "    adaptation_step=1e-5,  # eta\n",
    "    moment_term=0.9,  # alpha\n",
    "    number_of_iterations=500,  # iter\n",
    "    activation_function=np.tanh,\n",
    "):\n",
    "\n",
    "    # Each row for each input\n",
    "    # Each column for each hidden neuron\n",
    "    # Start the hidden layer weights with low random values\n",
    "    Wi = rd.randn(2, hidden_units) * .1\n",
    "    # print(Wi)\n",
    "\n",
    "    # Each row for each hidden neuron\n",
    "    b = np.zeros((hidden_units, 1))\n",
    "    # print(b)\n",
    "\n",
    "    # Each row for the output of each hidden neuron\n",
    "    # Start the output layer weights with low random values\n",
    "    Wo = rd.randn(hidden_units, 1) * .1\n",
    "    # print(Wo)\n",
    "\n",
    "    # Bias for the output layer\n",
    "    bo = 0\n",
    "\n",
    "    # Initialize the derivatives\n",
    "    dzWi = np.zeros(Wi.shape)\n",
    "    dzb = np.zeros(b.shape)\n",
    "    dzWo = np.zeros(Wo.shape)\n",
    "    dzbo = 0\n",
    "\n",
    "    Error = np.zeros(number_of_iterations)\n",
    "\n",
    "    for n in range(number_of_iterations):\n",
    "        # forward pass\n",
    "        u = np.dot(Wi.T, X) + b\n",
    "        v = activation_function(u)\n",
    "        z = np.dot(Wo.T, v) + bo\n",
    "        yh = activation_function(z)\n",
    "\n",
    "        # error\n",
    "        E = y-yh\n",
    "        Error[n] = np.mean(E**2)\n",
    "\n",
    "        # gradient\n",
    "        dz = -2*E*(1-yh**2)\n",
    "        dbo = np.sum(dz)\n",
    "        dWo = np.dot(v, dz.T)\n",
    "        du = (1-v**2)*np.dot(Wo, dz)\n",
    "        db = np.sum(du, axis=1)\n",
    "        dWi = np.dot(X, du.T)\n",
    "\n",
    "        # adapt\n",
    "        dzWi = moment_term * dzWi + dWi\n",
    "        dzb = moment_term * dzb + db[:, np.newaxis]\n",
    "        dzWo = moment_term * dzWo + dWo\n",
    "        dzbo = moment_term * dzbo + dbo\n",
    "\n",
    "        Wi = Wi - adaptation_step * dzWi\n",
    "        b = b - adaptation_step * dzb\n",
    "        Wo = Wo - adaptation_step * dzWo\n",
    "        bo = bo - adaptation_step * dzbo\n",
    "\n",
    "    return Error, yh, Wi, b, Wo, bo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network b) c) d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1, 10 and 50 hidden units\n",
    "for hu in [1, 10, 50]:\n",
    "    print(\"Hidden units:\", hu)\n",
    "    best = 1\n",
    "    best_error = []\n",
    "    best_yh = []\n",
    "    best_eta = 0\n",
    "    best_alpha = 0\n",
    "    best_Wi = []\n",
    "    best_b = []\n",
    "    best_Wo = []\n",
    "    best_bo = 0\n",
    "    for eta in np.linspace(1e-5, 1e-3, 10):\n",
    "        for alpha in np.linspace(0.9, 0.99, 10):\n",
    "            Error, yh, Wi, b, Wo, bo = train(X, y,\n",
    "                                             hidden_units=hu,\n",
    "                                             adaptation_step=eta,  # eta\n",
    "                                             moment_term=alpha,  # alpha\n",
    "                                             number_of_iterations=500,  # iter\n",
    "                                             )\n",
    "\n",
    "            if Error[len(Error)-1] < best:\n",
    "                best = Error[len(Error)-1]\n",
    "                best_error = Error.copy()\n",
    "                best_yh = yh.copy()\n",
    "                best_Wi = Wi.copy()\n",
    "                best_b = b.copy()\n",
    "                best_Wo = Wo.copy()\n",
    "                best_bo = bo\n",
    "                best_eta = eta\n",
    "                best_alpha = alpha\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(best_error)\n",
    "    plt.title('Error: ' + str(best) +\n",
    "              '\\nHidden Units: ' + str(hu) +\n",
    "              '\\neta: ' + str(best_eta) +\n",
    "              '\\nalpha: ' + str(best_alpha) +\n",
    "              '\\niter: ' + str(len(best_error)))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Training set errors\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ye = (best_yh.squeeze() >= 0.5)\n",
    "    plt.plot(X[0, y == 0], X[1, y == 0], '.b')\n",
    "    plt.plot(X[0, y == 1], X[1, y == 1], '.r')\n",
    "    plt.plot(X[0, y != ye], X[1, y != ye], 'ok', alpha=.7)\n",
    "    plt.show()\n",
    "    print('Nº of errors:', np.sum(y != ye))\n",
    "\n",
    "    # Wireframe\n",
    "\n",
    "    gX, gY = np.meshgrid(np.linspace(-1, 2, 50), np.linspace(-1, 2, 50))\n",
    "    X2 = np.vstack((gX.ravel(), gY.ravel()))\n",
    "    u = np.dot(best_Wi.T, X2)+best_b\n",
    "    v = np.tanh(u)\n",
    "    z = np.dot(best_Wo.T, v)\n",
    "    yh = np.tanh(z+best_bo)\n",
    "    Yh = np.reshape(yh, (50, 50))\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    # ax.contour3D(gX,gY,Yh,50,cmap='binary')\n",
    "    ax.plot_wireframe(gX, gY, Yh)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when we train with 1 hidden unit, the network is not able to learn the function because with only 1 hidden unit, there's no way differentiate the data with an intersection\n",
    "\n",
    "With 10 or 50 units we see not much difference, indicating that adding more hidden units is not usefull. Maybe a better result could be achieved increasing the number of hidden layers instead of the number of hidden units\n",
    "\n",
    "We can see that the best alpha is close to 1 because at the end of the iterations, the network was able to reach that error value relatively fast compared to other alpha values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from skimage import color\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Xtrain = Xtrain.astype('float32')\n",
    "# Xtest = Xtest.astype('float32')\n",
    "# Xtrain /= 255\n",
    "# Xtest /= 255\n",
    "\n",
    "# Transform data into array from 1-col matrix\n",
    "ytrain = ytrain.squeeze()\n",
    "ytest = ytest.squeeze()\n",
    "\n",
    "print(Xtrain.shape, ytrain.shape)\n",
    "print(Xtest.shape, ytest.shape)\n",
    "\n",
    "# Randomize train data\n",
    "idx = rd.permutation(Xtrain.shape[0])\n",
    "Xtrain = Xtrain[idx]\n",
    "ytrain = ytrain[idx]\n",
    "\n",
    "# Sort test data\n",
    "idx = np.argsort(ytest)\n",
    "Xtest = Xtest[idx]\n",
    "ytest = ytest[idx]\n",
    "\n",
    "# Convert to grayscale\n",
    "XtrainG = np.zeros((Xtrain.shape[0], 32, 32))\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    XtrainG[i] = color.rgb2gray(Xtrain[i])\n",
    "\n",
    "XtestG = np.zeros((Xtest.shape[0], 32, 32))\n",
    "for i in range(Xtest.shape[0]):\n",
    "    XtestG[i] = color.rgb2gray(Xtest[i])\n",
    "\n",
    "# Convert to vector\n",
    "XtrainGV = XtrainG.reshape((XtrainG.shape[0], XtrainG.shape[1] ** 2)) * 1.0\n",
    "XtestGV = XtestG.reshape((XtestG.shape[0], XtestG.shape[1] ** 2)) * 1.0\n",
    "\n",
    "# Categorical\n",
    "ytrainC = keras.utils.to_categorical(ytrain)\n",
    "ytestC = keras.utils.to_categorical(ytest)\n",
    "\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(\n",
    "    XtrainGV, ytrainC, test_size=0.1)\n",
    "\n",
    "# Pre-processing\n",
    "# ss = StandardScaler().fit(XtrainGV)\n",
    "# XtrainGVp = ss.transform(XtrainGV)\n",
    "# XtestGVp = ss.transform(XtestGV)\n",
    "# X1_trainP = ss.transform(X1_train)\n",
    "# X1_valP = ss.transform(X1_val)\n",
    "\n",
    "n_components = 250\n",
    "pca = PCA(n_components=n_components).fit(XtrainGV)\n",
    "XtrainGVp = pca.transform(XtrainGV)\n",
    "XtestGVp = pca.transform(XtestGV)\n",
    "X1_trainP = pca.transform(X1_train)\n",
    "X1_valP = pca.transform(X1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = XtrainGV\n",
    "y1 = ytrain\n",
    "y1C = ytrainC\n",
    "\n",
    "X1p = XtrainGVp\n",
    "X1s = X1_train\n",
    "X1sp = X1_trainP\n",
    "X1sVal = X1_val\n",
    "X1sValP = X1_valP\n",
    "y1s = y1_train\n",
    "y1sVal = y1_val\n",
    "\n",
    "X2 = XtestGV\n",
    "X2p = XtestGVp\n",
    "y2 = ytest\n",
    "y2C = ytestC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = keras.Sequential()\n",
    "nn.add(keras.layers.Flatten(input_shape=(32*32,)))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "nn.compile(optimizer=\"nadam\",\n",
    "           loss=keras.losses.categorical_crossentropy,\n",
    "           metrics=[\"accuracy\"],\n",
    "           )\n",
    "\n",
    "print(nn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nn.fit(X1, y1C,\n",
    "               epochs=50,\n",
    "               batch_size=1000,\n",
    "               validation_data=(X2, y2C),\n",
    "               use_multiprocessing=True,\n",
    "               )\n",
    "\n",
    "h = train.history\n",
    "plt.plot(h[\"loss\"], label='Loss')\n",
    "plt.plot(h[\"accuracy\"], label='Accuracy')\n",
    "plt.plot(h[\"val_loss\"], label='Val Loss')\n",
    "plt.plot(h[\"val_accuracy\"], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2e = nn.predict(X2)\n",
    "y2p = np.argmax(y2e, axis=1)\n",
    "\n",
    "print('Nº of errors:', np.sum(y2 != y2p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the accuracy increasing, the val_accuracy doesn't improve, indicating that the network stopped learning for new inputs and is now memorizing the training data\n",
    "We confirm that seeing that the val_loss started increasing a little bit at 50 epochs +-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 512\n",
    "dropout_rate = 0.5\n",
    "nn = keras.Sequential()\n",
    "nn.add(keras.layers.Flatten(input_shape=(n_components,)))\n",
    "nn.add(keras.layers.Dense(hidden_units, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dropout(dropout_rate))\n",
    "nn.add(keras.layers.Dense(hidden_units, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dropout(dropout_rate))\n",
    "nn.add(keras.layers.Dense(hidden_units, activation=\"relu\"))\n",
    "nn.add(keras.layers.Dropout(dropout_rate))\n",
    "nn.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "nn.compile(optimizer=\"nadam\",\n",
    "           loss=keras.losses.categorical_crossentropy,\n",
    "           metrics=[\"accuracy\"],\n",
    "           )\n",
    "\n",
    "# print(nn.summary())\n",
    "\n",
    "train = nn.fit(X1sp, y1s,\n",
    "               epochs=50,\n",
    "               batch_size=10000,\n",
    "               validation_data=(X1sValP, y1sVal),\n",
    "               use_multiprocessing=True,\n",
    "               )\n",
    "\n",
    "h = train.history\n",
    "plt.plot(h[\"loss\"], label='Loss')\n",
    "plt.plot(h[\"accuracy\"], label='Accuracy')\n",
    "plt.plot(h[\"val_loss\"], label='Val Loss')\n",
    "plt.plot(h[\"val_accuracy\"], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2e = nn.predict(X2p)\n",
    "y2p = np.argmax(y2e, axis=1)\n",
    "\n",
    "print('Nº of errors:', np.sum(y2 != y2p))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5be1fd1de7b40782a9e00e64e3cbb4863fd6c2414af64edfcd9ccb8133f2c5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
